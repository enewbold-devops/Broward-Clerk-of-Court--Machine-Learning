{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9229d3f6",
   "metadata": {},
   "source": [
    "# Case Events from Judge Favorable Dispositions\n",
    "    Judge×Charge Event Playbooks.” It sticks to your CSV schemas and outputs three artifacts:\n",
    "        1. rates_by_judge_charge – favorable rates by Judge_Name × current_offense_description\n",
    "        2. tokens_by_judge_charge_topK – top event tokens (lift vs. each judge×charge baseline) within 180 days pre-Disposition_Date\n",
    "        3. bigrams_by_judge_charge_topK – top event bigrams (likely orders of actions) in that same window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9fb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import Window as W\n",
    "from delta import *\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\\hadoop-3.4.0\"\n",
    "os.environ[\"hadoop.home.dir\"] = r\"C:\\hadoop\\hadoop-3.4.0\"\n",
    "\n",
    "blobacct = os.getenv(\"AZURE_DATALAKE_ACCOUNT_NAME\")\n",
    "accesskey = os.getenv(\"AZURE_DATALAKE_ACCOUNT_KEY\")\n",
    "\n",
    "bronze_db = os.getenv(\"LOCAL_DELTA_BRONZE_DB\")\n",
    "silver_db = os.getenv(\"LOCAL_DELTA_SILVER_DB\")\n",
    "\n",
    "clerkSession = SparkSession.builder.appName(\"Clerk_Favorable_Outcome_Events_Analysis_Bronze\")\\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.azurebfs.impl\", \"org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "          \"io.delta:delta-spark_2.12:3.2.0,\"\n",
    "          \"org.apache.hadoop:hadoop-azure:3.4.0,\"\n",
    "          \"org.apache.hadoop:hadoop-common:3.4.0\") \\\n",
    "    .config(f\"fs.azure.account.key.{blobacct}.dfs.core.windows.net\", accesskey) \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "sparkClerk = configure_spark_with_delta_pip(clerkSession).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716ff22",
   "metadata": {},
   "source": [
    "### Load the Delta Tables into Spark Dataframes\n",
    "    setup cofiguration veriables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1345d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|         Case_Number|   string|   NULL|\n",
      "|State_Reporting_N...|   string|   NULL|\n",
      "|             Caption|   string|   NULL|\n",
      "|           Case_Type|   string|   NULL|\n",
      "|      Case_Type_Code|   string|   NULL|\n",
      "|          Court_Type|   string|   NULL|\n",
      "|     Court_Type_Code|   string|   NULL|\n",
      "|          Filed_Date|     date|   NULL|\n",
      "|        Created_Date|     date|   NULL|\n",
      "|    Disposition_Code|   string|   NULL|\n",
      "|    Disposition_Date|     date|   NULL|\n",
      "|  Disposition_Status|   string|   NULL|\n",
      "|      Court_Location|   string|   NULL|\n",
      "|     Magistrate_Name|   string|   NULL|\n",
      "|          Judge_Name|   string|   NULL|\n",
      "|                BCCN|   string|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "\n",
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|        Offense_Date|     date|   NULL|\n",
      "|       Charge_Number|   string|   NULL|\n",
      "|Current_Offense_S...|   string|   NULL|\n",
      "|Current_Offense_D...|   string|   NULL|\n",
      "|Current_Offense_D...|   string|   NULL|\n",
      "|          Filed_Date|     date|   NULL|\n",
      "|         Filing_Type|   string|   NULL|\n",
      "|       Filing_Agency|   string|   NULL|\n",
      "|Original_Offense_...|   string|   NULL|\n",
      "|Original_Offense_...|   string|   NULL|\n",
      "|Original_Offense_...|   string|   NULL|\n",
      "|     Citation_Number|   string|   NULL|\n",
      "|Citation_Speed_Am...|   string|   NULL|\n",
      "|Citation_Posted_A...|   string|   NULL|\n",
      "|Citation_Vehicle_...|   string|   NULL|\n",
      "|Citation_Vehicle_...|   string|   NULL|\n",
      "|Citation_Vehicle_...|   string|   NULL|\n",
      "|Citation_Vehicle_...|   string|   NULL|\n",
      "|         Case_Number|   string|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "\n",
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|        Case_Number|   string|   NULL|\n",
      "|      Charge_Number|   string|   NULL|\n",
      "|       Offense_Date|   string|   NULL|\n",
      "|       Plea_Summary|   string|   NULL|\n",
      "|Disposition_Summary|   string|   NULL|\n",
      "|   Sentence_Summary|   string|   NULL|\n",
      "+-------------------+---------+-------+\n",
      "\n",
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|      Description|   string|   NULL|\n",
      "|      EventAmount|   string|   NULL|\n",
      "|   AdditionalText|   string|   NULL|\n",
      "|        Date2Desc|   string|   NULL|\n",
      "|            Date2|   string|   NULL|\n",
      "|        PartyList|   string|   NULL|\n",
      "|       Party2List|   string|   NULL|\n",
      "|        CauseList|   string|   NULL|\n",
      "|EventDocumentList|   string|   NULL|\n",
      "|      Case_Number|   string|   NULL|\n",
      "|   EventDocuments|   string|   NULL|\n",
      "|        EventDate|     date|   NULL|\n",
      "+-----------------+---------+-------+\n",
      "\n",
      "root\n",
      " |-- Description: string (nullable = true)\n",
      " |-- EventAmount: string (nullable = true)\n",
      " |-- AdditionalText: string (nullable = true)\n",
      " |-- Date2Desc: string (nullable = true)\n",
      " |-- Date2: string (nullable = true)\n",
      " |-- PartyList: string (nullable = true)\n",
      " |-- Party2List: string (nullable = true)\n",
      " |-- CauseList: string (nullable = true)\n",
      " |-- EventDocumentList: string (nullable = true)\n",
      " |-- Case_Number: string (nullable = true)\n",
      " |-- EventDocuments: string (nullable = true)\n",
      " |-- EventDate: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Case_Number: string (nullable = true)\n",
      " |-- State_Reporting_Number: string (nullable = true)\n",
      " |-- Caption: string (nullable = true)\n",
      " |-- Case_Type: string (nullable = true)\n",
      " |-- Case_Type_Code: string (nullable = true)\n",
      " |-- Court_Type: string (nullable = true)\n",
      " |-- Court_Type_Code: string (nullable = true)\n",
      " |-- Filed_Date: date (nullable = true)\n",
      " |-- Created_Date: date (nullable = true)\n",
      " |-- Disposition_Code: string (nullable = true)\n",
      " |-- Disposition_Date: date (nullable = true)\n",
      " |-- Disposition_Status: string (nullable = true)\n",
      " |-- Court_Location: string (nullable = true)\n",
      " |-- Magistrate_Name: string (nullable = true)\n",
      " |-- Judge_Name: string (nullable = true)\n",
      " |-- BCCN: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Offense_Date: date (nullable = true)\n",
      " |-- Charge_Number: string (nullable = true)\n",
      " |-- Current_Offense_Statute: string (nullable = true)\n",
      " |-- Current_Offense_Degree: string (nullable = true)\n",
      " |-- Current_Offense_Description: string (nullable = true)\n",
      " |-- Filed_Date: date (nullable = true)\n",
      " |-- Filing_Type: string (nullable = true)\n",
      " |-- Filing_Agency: string (nullable = true)\n",
      " |-- Original_Offense_Statute: string (nullable = true)\n",
      " |-- Original_Offense_Degree: string (nullable = true)\n",
      " |-- Original_Offense_Description: string (nullable = true)\n",
      " |-- Citation_Number: string (nullable = true)\n",
      " |-- Citation_Speed_Amount: string (nullable = true)\n",
      " |-- Citation_Posted_Amount: string (nullable = true)\n",
      " |-- Citation_Vehicle_License_Number: string (nullable = true)\n",
      " |-- Citation_Vehicle_License_State: string (nullable = true)\n",
      " |-- Citation_Vehicle_Year: string (nullable = true)\n",
      " |-- Citation_Vehicle_Model: string (nullable = true)\n",
      " |-- Case_Number: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Case_Number: string (nullable = true)\n",
      " |-- Charge_Number: string (nullable = true)\n",
      " |-- Offense_Date: string (nullable = true)\n",
      " |-- Plea_Summary: string (nullable = true)\n",
      " |-- Disposition_Summary: string (nullable = true)\n",
      " |-- Sentence_Summary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymodules.CaseDeltaLake as cdl\n",
    "\n",
    "WINDOW_DAYS = 180\n",
    "TOP_K_PER_GROUP = 15\n",
    "MIN_GROUP_CASES = 10 # stability filter: min total cases per (Judge, Charge)\n",
    "MIN_PATTERN_CASES= 3 # stability filter: token/bigram must appear in ≥ this many cases\n",
    "\n",
    "case_df = cdl.CaseDeltaLake().delta_table_schema(\"cases\", sparkClerk)\n",
    "charge_df = cdl.CaseDeltaLake().delta_table_schema(\"charges\", sparkClerk)\n",
    "disposition_df = cdl.CaseDeltaLake().delta_table_schema(\"dispositions\", sparkClerk)\n",
    "event_df = cdl.CaseDeltaLake().delta_table_schema(\"events\", sparkClerk)\n",
    "\n",
    "event_df.printSchema()\n",
    "case_df.printSchema()\n",
    "charge_df.printSchema()\n",
    "disposition_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a78062",
   "metadata": {},
   "source": [
    "### NORMALIZATIONS & FLAGS\n",
    "    charges: felony flag from current_offense_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db240bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not {\"Case_Number\",\"Charge_Number\",\"Current_Offense_Degree\",\"Current_Offense_Description\"}.issubset(set(charge_df.columns)):\n",
    "    missing = {\"Case_Number\",\"Charge_Number\",\"Current_Offense_Degree\",\"Current_Offense_Description\"} - set(charge_df.columns)\n",
    "    raise ValueError(f\"Delta Table for charge_df missing required columns: {missing}\")\n",
    "\n",
    "charge_df = charge_df.withColumn(\n",
    "    \"is_felony\",\n",
    "    (\n",
    "        F.upper(F.col(\"Current_Offense_Degree\")).contains(\"(F\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# dispositions: favorable flag from disposition_summary\n",
    "if not {\"Case_Number\",\"Charge_Number\",\"Disposition_Summary\"}.issubset(set(disposition_df.columns)):\n",
    "    missing = {\"Case_Number\",\"Charge_Number\",\"Disposition_Summary\"} - set(disposition_df.columns)\n",
    "    raise ValueError(f\"Delta Table for disposition_df missing required columns: {missing}\")\n",
    "\n",
    "fav_regex =v_regex = \"ADJUDICATION WITHHELD|WITHHELD|NOT GUILTY|ACQUITTED|DISMISSED|NOLLE PROSEQUI|NOL PROS\"\n",
    "disposition_df = disposition_df.withColumn(\"disposition_upper\", F.upper(F.col(\"Disposition_Summary\")))\n",
    "disposition_df = disposition_df.withColumn(\"is_favorable\", F.col(\"disposition_upper\").rlike(v_regex))\n",
    "\n",
    "# cases: must have Case_Number, Judge_Name, Disposition_Date\n",
    "if not {\"Case_Number\",\"Judge_Name\",\"Disposition_Date\"}.issubset(set(case_df.columns)):\n",
    "    missing = {\"Case_Number\",\"Judge_Name\",\"Disposition_Date\"} - set(case_df.columns)\n",
    "    raise ValueError(f\"Delta Table for case_df missing required columns: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a0690",
   "metadata": {},
   "source": [
    "### BUILD CHARGE-LEVEL DATASET (join charges + dispositions + cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0877b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = (charge_df\n",
    "      .join(disposition_df.select(\"Case_Number\",\"Charge_Number\",\"is_favorable\"), [\"Case_Number\",\"Charge_Number\"], \"left\")\n",
    "      .join(case_df.select(\"Case_Number\",\"Judge_Name\",\"Disposition_Date\"), charge_df.Case_Number == case_df.Case_Number, \"left\")\n",
    "     )\n",
    "\n",
    "# Focus on felony charges only\n",
    "felony = cd.filter(F.col(\"is_felony\") == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae2419",
   "metadata": {},
   "source": [
    "### FAVORABLE RATES BY JUDGE × CHARGE DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2d8080",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_by_judge_charge = (\n",
    "    felony.groupBy(\"Judge_Name\",\"Current_Offense_Description\", \"Current_Offense_Statute\")\n",
    "    .agg(\n",
    "        F.count(F.lit(1)).alias(\"total_charges\"),\n",
    "        F.sum(F.when(F.col(\"is_favorable\") == True, 1).otherwise(0)).alias(\"favorable\")\n",
    "    )\n",
    "    .withColumn(\"favorable_rate\", F.col(\"favorable\")/F.col(\"total_charges\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9837bb2",
   "metadata": {},
   "source": [
    "### 180-DAY PRE-DISPOSITION EVENT WINDOW & TOKENIZATION\n",
    "    Token priority: eventdocuments, then description, then additionaltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf54d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|      Description|   string|   NULL|\n",
      "|      EventAmount|   string|   NULL|\n",
      "|   AdditionalText|   string|   NULL|\n",
      "|        Date2Desc|   string|   NULL|\n",
      "|            Date2|   string|   NULL|\n",
      "|        PartyList|   string|   NULL|\n",
      "|       Party2List|   string|   NULL|\n",
      "|        CauseList|   string|   NULL|\n",
      "|EventDocumentList|   string|   NULL|\n",
      "|      Case_Number|   string|   NULL|\n",
      "|   EventDocuments|   string|   NULL|\n",
      "|        EventDate|     date|   NULL|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "event_df = event_df.withColumn(\"EventDocuments\", F.regexp_replace(col(\"EventDocuments\"), r\"\\|\", \"\"))\n",
    "event_df = event_df.withColumn(\"EventDocuments\", F.regexp_replace(col(\"EventDocuments\"), \"eFile:\", \"\"))\n",
    "\n",
    "if not {\"Case_Number\",\"EventDate\"}.issubset(set(event_df.columns)):\n",
    "    raise ValueError(\"event_df.csv must have 'Case_Number' and parsable 'EventDate'\")\n",
    "\n",
    "pref = F.when(F.length(F.col(\"EventDocuments\")) > 0, F.col(\"EventDocuments\")) \\\n",
    "         .otherwise(F.when(F.length(F.col(\"Description\")) > 0, F.col(\"Description\")))\n",
    "\n",
    "events_tok = (\n",
    "    event_df\n",
    "    .withColumn(\"token\", F.upper(F.trim(pref)))\n",
    "    .filter(F.length(\"token\") > 1)\n",
    ")\n",
    "\n",
    "# Join judge + Disposition_TS onto events\n",
    "evj = (\n",
    "    events_tok\n",
    "    .join(case_df\n",
    "          .select(\"Case_Number\",\"Judge_Name\",\"Disposition_Date\"),\n",
    "          events_tok.Case_Number == case_df.Case_Number,\n",
    "          \"left\")\n",
    ")\n",
    "\n",
    "evj = evj.drop(case_df.Case_Number)\n",
    "\n",
    "# Filter to 180-day window: event_ts ∈ [Disposition_TS - 180d, Disposition_TS]\n",
    "ev_win = (\n",
    "    evj.select(\"Case_Number\", \"Judge_Name\", \"token\", \"EventDate\", \"Disposition_Date\")\n",
    "    .filter((F.col(\"Disposition_Date\").isNotNull()) & (F.col(\"EventDate\").isNotNull()))\n",
    "    .filter(F.col(\"EventDate\") <= F.col(\"Disposition_Date\"))\n",
    "    .filter(F.col(\"EventDate\") >= F.col(\"Disposition_Date\") - F.expr(f\"INTERVAL {WINDOW_DAYS} DAYS\"))\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81bad4f",
   "metadata": {},
   "source": [
    "### CASE-LEVEL OUTCOMES BY (case, Judge, charge_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2a28af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "felony = felony.drop(case_df.Case_Number)\n",
    "# CASE-LEVEL OUTCOMES BY (case, Judge, charge_description)\n",
    "case_charge_outcomes = (\n",
    "    felony.select(\"Case_Number\", \"Judge_Name\", \"Current_Offense_Description\", \"Current_Offense_Statute\", \"is_favorable\")\n",
    "          .groupBy(\"Case_Number\",\"Judge_Name\",\"Current_Offense_Description\", \"Current_Offense_Statute\")\n",
    "          .agg(F.max(F.col(\"is_favorable\").cast(\"int\")).alias(\"is_fav_int\"))\n",
    "          .withColumn(\"is_favorable\", (F.col(\"is_fav_int\") == 1))\n",
    "          .drop(\"is_fav_int\")\n",
    ")\n",
    "\n",
    "# Baselines: per (Judge, charge_description)\n",
    "base = (\n",
    "    case_charge_outcomes\n",
    "    .groupBy(\"Judge_Name\",\"Current_Offense_Description\", \"Current_Offense_Statute\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"Case_Number\").alias(\"total_cases\"),\n",
    "        F.sum(F.when(F.col(\"is_favorable\") == True, 1).otherwise(0)).alias(\"fav_cases\")\n",
    "    )\n",
    "    .withColumn(\"p_fav\", F.col(\"fav_cases\")/F.col(\"total_cases\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e70ce",
   "metadata": {},
   "source": [
    "### TOKEN LIFT by (Judge, charge_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c0ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window as W\n",
    "\n",
    "# 1) Unique (Case, token) within your event window\n",
    "case_token = ev_win.select(\"Case_Number\", \"token\").dropDuplicates()\n",
    "\n",
    "# 2) Attach case-level attributes and outcomes\n",
    "ct_jc = (\n",
    "    case_token\n",
    "    .join(case_charge_outcomes, \"Case_Number\", \"left\")\n",
    "    .select(\n",
    "        \"Case_Number\",\n",
    "        \"Judge_Name\",\n",
    "        \"Current_Offense_Description\",\n",
    "        \"Current_Offense_Statute\",\n",
    "        \"token\",\n",
    "        \"is_favorable\"\n",
    "    )\n",
    "    .dropDuplicates([\n",
    "        \"Case_Number\",\n",
    "        \"Judge_Name\",\n",
    "        \"Current_Offense_Description\",\n",
    "        \"Current_Offense_Statute\",\n",
    "        \"token\"\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 3) Counts per (Judge, Offense, Statute, token)\n",
    "tok_counts = (\n",
    "    ct_jc.groupBy(\"Judge_Name\", \"Current_Offense_Description\", \"Current_Offense_Statute\", \"token\")\n",
    "         .agg(\n",
    "             F.countDistinct(\"Case_Number\").alias(\"cases_with_pattern\"),\n",
    "             F.coalesce(F.sum(F.col(\"is_favorable\").cast(\"int\")), F.lit(0)).alias(\"fav_cases_with_pattern\")\n",
    "         )\n",
    ")\n",
    "\n",
    "# 4) Join baselines and compute conditional p and lift (guarded)\n",
    "# Expect `base` to have: Judge_Name, Current_Offense_Description, Current_Offense_Statute, total_cases, p_fav\n",
    "tok_stats = (\n",
    "    tok_counts\n",
    "    # .join(F.broadcast(base), [\"Judge_Name\",\"Current_Offense_Description\",\"Current_Offense_Statute\"], \"left\")  # enable if base is small\n",
    "    .join(base, [\"Judge_Name\",\"Current_Offense_Description\",\"Current_Offense_Statute\"], \"left\")\n",
    "    .withColumn(\n",
    "        \"p_fav_given_pattern\",\n",
    "        F.when(F.col(\"cases_with_pattern\") > 0,\n",
    "               F.col(\"fav_cases_with_pattern\").cast(\"double\") / F.col(\"cases_with_pattern\").cast(\"double\"))\n",
    "         .otherwise(F.lit(None).cast(\"double\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"lift\",\n",
    "        F.when((F.col(\"p_fav\").isNotNull()) & (F.col(\"p_fav\") > F.lit(0)),\n",
    "               F.col(\"p_fav_given_pattern\") / F.col(\"p_fav\").cast(\"double\"))\n",
    "         .otherwise(F.lit(None).cast(\"double\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5) Stability filters\n",
    "stable_tok = tok_stats.filter(\n",
    "    (F.col(\"total_cases\") >= F.lit(MIN_GROUP_CASES)) &\n",
    "    (F.col(\"cases_with_pattern\") >= F.lit(MIN_PATTERN_CASES))\n",
    ")\n",
    "\n",
    "# 6) Rank within each (Judge, Offense, Statute)\n",
    "w_tok = W.partitionBy(\"Judge_Name\",\"Current_Offense_Description\",\"Current_Offense_Statute\") \\\n",
    "         .orderBy(\n",
    "             F.col(\"lift\").desc_nulls_last(),\n",
    "             F.col(\"p_fav_given_pattern\").desc_nulls_last(),\n",
    "             F.col(\"fav_cases_with_pattern\").desc(),\n",
    "             F.col(\"cases_with_pattern\").desc()\n",
    "         )\n",
    "\n",
    "ranked_tok = (\n",
    "    stable_tok\n",
    "    .withColumn(\"rank\", F.row_number().over(w_tok))\n",
    "    .filter(F.col(\"rank\") <= F.lit(TOP_K_PER_GROUP))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820bbd6",
   "metadata": {},
   "source": [
    "### BIGRAM LIFT by (Judge, charge_description)\n",
    "    For bigrams, we need per‑case, time‑ordered tokens within the window joined to charge groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6521a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window as W\n",
    "\n",
    "# Aliases\n",
    "ev = ev_win.alias(\"ev\")\n",
    "cco = case_charge_outcomes.select(\n",
    "    \"Case_Number\",\n",
    "    \"Judge_Name\",\n",
    "    \"Current_Offense_Description\",\n",
    "    \"Current_Offense_Statute\",\n",
    "    \"is_favorable\"\n",
    ").alias(\"cco\")\n",
    "\n",
    "# 1) Join with explicit column projection to avoid collisions\n",
    "ev_win_jc = (\n",
    "    ev.join(cco, on=\"Case_Number\", how=\"left\")\n",
    "      .select(\n",
    "          \"ev.Case_Number\",\n",
    "          \"ev.token\",\n",
    "          \"ev.EventDate\",            # ensure timestamp\n",
    "          # add a stable tie-breaker if available (EventTS/EventId)\n",
    "          # \"ev.EventTS\",\n",
    "          \"cco.Judge_Name\",\n",
    "          \"cco.Current_Offense_Description\",\n",
    "          \"cco.Current_Offense_Statute\",\n",
    "          \"cco.is_favorable\"\n",
    "      )\n",
    ")\n",
    "\n",
    "# Optional: enforce types + deterministic ordering column\n",
    "ev_win_jc = (\n",
    "    ev_win_jc\n",
    "    .withColumn(\"EventDate\", F.col(\"EventDate\").cast(\"timestamp\"))\n",
    "    # .withColumn(\"EventTS\", F.col(\"EventTS\").cast(\"timestamp\"))\n",
    ")\n",
    "\n",
    "# 2) Build bigrams with stable ordering per (case, judge, charge)\n",
    "order_cols = [\"EventDate\"]  # replace with [\"EventDate\", \"EventTS\"] or [\"EventDate\",\"EventId\"] if available\n",
    "w = W.partitionBy(\n",
    "        \"Case_Number\",\"Judge_Name\",\"Current_Offense_Description\",\"Current_Offense_Statute\"\n",
    "    ).orderBy(*order_cols)\n",
    "\n",
    "ev_with_next = ev_win_jc.withColumn(\"next_token\", F.lead(\"token\").over(w))\n",
    "\n",
    "bigrams = (\n",
    "    ev_with_next\n",
    "    .filter(F.col(\"next_token\").isNotNull())\n",
    "    .withColumn(\"bigram\", F.concat_ws(\" | \", F.col(\"token\"), F.col(\"next_token\")))\n",
    "    .select(\n",
    "        \"Case_Number\",\n",
    "        \"Judge_Name\",\n",
    "        \"Current_Offense_Description\",\n",
    "        \"Current_Offense_Statute\",\n",
    "        \"bigram\",\n",
    "        \"is_favorable\"\n",
    "    )\n",
    "    # Presence of bigram in a case (avoid multiple counts per case)\n",
    "    .dropDuplicates([\n",
    "        \"Case_Number\",\n",
    "        \"Judge_Name\",\n",
    "        \"Current_Offense_Description\",\n",
    "        \"Current_Offense_Statute\",\n",
    "        \"bigram\"\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 3) Aggregate counts per group\n",
    "bg_counts = (\n",
    "    bigrams.groupBy(\"Judge_Name\",\"Current_Offense_Description\",\"Current_Offense_Statute\",\"bigram\")\n",
    "           .agg(\n",
    "               F.countDistinct(\"Case_Number\").alias(\"cases_with_pattern\"),\n",
    "               F.coalesce(F.sum(F.col(\"is_favorable\").cast(\"int\")), F.lit(0)).alias(\"fav_cases_with_pattern\")\n",
    "           )\n",
    ")\n",
    "\n",
    "# 4) Join baseline and compute probabilities + lift (null/zero-safe)\n",
    "# base must contain: Judge_Name, Current_Offense_Description, Current_Offense_Statute, total_cases, p_fav\n",
    "bg_stats = (\n",
    "    bg_counts\n",
    "    # .join(F.broadcast(base), [\"Judge_Name\",\"Current_Offense_Description\",\"Current_Offense_Statute\"], \"left\")\n",
    "    .join(base, [\"Judge_Name\",\"Current_Offense_Description\",\"Current_Offense_Statute\"], \"left\")\n",
    "    .withColumn(\n",
    "        \"p_fav_given_pattern\",\n",
    "        F.when(F.col(\"cases_with_pattern\") > 0,\n",
    "               F.col(\"fav_cases_with_pattern\").cast(\"double\") /\n",
    "               F.col(\"cases_with_pattern\").cast(\"double\"))\n",
    "         .otherwise(F.lit(None).cast(\"double\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"lift\",\n",
    "        F.when((F.col(\"p_fav\").isNotNull()) & (F.col(\"p_fav\") > F.lit(0)),\n",
    "               F.col(\"p_fav_given_pattern\") / F.col(\"p_fav\").cast(\"double\"))\n",
    "         .otherwise(F.lit(None).cast(\"double\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5) Stability thresholds\n",
    "stable_bg = bg_stats.filter(\n",
    "    (F.col(\"total_cases\") >= F.lit(MIN_GROUP_CASES)) &\n",
    "    (F.col(\"cases_with_pattern\") >= F.lit(MIN_PATTERN_CASES))\n",
    ")\n",
    "\n",
    "# 6) Rank top-K per (Judge, Offense, Statute)\n",
    "w_bg = W.partitionBy(\"Judge_Name\",\"Current_Offense_Description\",\"Current_Offense_Statute\").orderBy(\n",
    "    F.col(\"lift\").desc_nulls_last(),\n",
    "    F.col(\"p_fav_given_pattern\").desc_nulls_last(),\n",
    "    F.col(\"fav_cases_with_pattern\").desc(),\n",
    "    F.col(\"cases_with_pattern\").desc()\n",
    ")\n",
    "\n",
    "ranked_bg = (\n",
    "    stable_bg\n",
    "    .withColumn(\"rank\", F.row_number().over(w_bg))\n",
    "    .filter(F.col(\"rank\") <= F.lit(TOP_K_PER_GROUP))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed19371d",
   "metadata": {},
   "source": [
    "### Write Event Tokens by Judge and Charge by Favorable Cases - to Azure Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00fa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame name: _delta_views/vw_EventsRankedByToken\n",
      "DataFrame name: _delta_views/vw_EventsRankedByBigram\n"
     ]
    }
   ],
   "source": [
    "import pymodules.CaseDeltaLake as cdl\n",
    "\n",
    "cdl.CaseDeltaLake().write_delta_bronze(ranked_tok, \"_delta_views/vw_EventsRankedByToken\", sparkClerk)\n",
    "\n",
    "cdl.CaseDeltaLake().write_delta_bronze(ranked_bg, \"_delta_views/vw_EventsRankedByBigram\", sparkClerk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
